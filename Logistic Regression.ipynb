{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "By Rama Vempati\n",
    "\n",
    "To connect with or follow me on LinkedIn [Click Here](https://www.linkedin.com/in/ramavempati/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "* Logistic Regression is one of the basic and popular algorithm to solve a classification problem. \n",
    "\n",
    "\n",
    "* It is named as ‘Logistic Regression’, because it’s underlying technique is quite the same as Linear Regression. \n",
    "\n",
    "\n",
    "* The term “Logistic” is taken from the Logit function that is used in this method of classification.\n",
    "\n",
    "\n",
    "* Logistic Regression is used when the dependent variable(target) is categorical (you need to encode the categorical variable into a numeric value)\n",
    "\n",
    "\n",
    "* For multi class problems you can use Multinomial Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is it used?\n",
    "\n",
    "Logistic Regression can be used for various classification problems such as \n",
    "1. Spam detection\n",
    "2. Diabetes prediction\n",
    "3. If a given customer will purchase a particular product\n",
    "4. If a customer will churn to another competitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why not use Linear Regression?\n",
    "\n",
    "Consider an example of where we have data of tumor size vs its malignancy. As it is a classification problem that either a patient is maligant or not. If we plot, we can see, all the values will lie on 0 and 1. And if we fit best found regression line, by assuming the threshold at 0.5, the graph might look like below\n",
    "\n",
    "<img src='images/logit1.png' alt='logit1' style=\"width: 500px;\"/>\n",
    "\n",
    "We can decide the point on the x axis from where all the values lie to its left side are considered as negative class and all the values lie to its right side are positive class.\n",
    "\n",
    "<img src='images/logit2.jpeg' alt='logit2' style=\"width: 500px;\"/>\n",
    "\n",
    "But what if there is an outlier in the data. Things would get pretty messy. For example, for 0.5 threshold.\n",
    "\n",
    "<img src='images/logit3.png' alt='logit3' style=\"width: 500px;\"/>\n",
    "\n",
    "If we fit best found regression line, it still won’t be enough to decide any point by which we can differentiate classes. It will put some positive class examples into negative class. The green dotted line (Decision Boundary) is dividing malignant tumors from benign tumors but the line should have been at a yellow line which is clearly dividing the positive and negative examples. So just a single outlier is disturbing the whole linear regression predictions. And that is where logistic regression comes into a picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "As discussed earlier, to deal with outliers, Logistic Regression uses Sigmoid function.\n",
    "An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a Sigmoid function, which takes any real value between zero and one.\n",
    "\n",
    "Step functions enforce some threshold on a variable e.g.: discard anything below a certain value. \n",
    "\n",
    "$$u(x) \\begin{cases} 0 & \\mbox{if} & x < 0 \\  \\\\ 1 & \\mbox{if} & x \\geq 0 \\ \\end{cases}$$\n",
    "\n",
    "Step functions are like switches. We could utilise them to trigger a signal whenever a threshold is crossed. Imagine opening the floodgates whenever the water level crosses a certain point or turning of the electric lighting when the ambient light brightness exceeds a given measure.\n",
    "\n",
    "Perceptrons, binary classifiers and the neurons in artificial neural networks require an activation function in order to produce any output. The step function is a valid option for the activation function but poses a challenge in analysis because of the jump disconuity at $x=0$.\n",
    "\n",
    "At $x=0$ the derivative is undefined ($\\frac{\\infty}{0}$) while the derivative is zero for the remainder of the domain.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "The sigmoid function $\\sigma(x)$, because of its differentiability, is sometimes used as an alternative to the step function $u(x)$.\n",
    "\n",
    "Some may choose to introduce a weight $\\beta$ to the input variables in order to obtain a sigmoid curve more reminiscent of the step we're trying to mimic. By tweaking the $\\beta$ term, one can obtain a result that approaches the step function, yet remains differentiable throughout its entire domain.\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x\\beta}}$$\n",
    "\n",
    "\n",
    "<img src='images/logit4.png' alt='logit4' style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Logistic Regression:\n",
    "\n",
    "* The dependent variable in logistic regression follows Bernoulli Distribution.\n",
    "\n",
    "\n",
    "* Estimation is done through maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "The MLE is a \"likelihood\" maximization method, while OLS (used in Linear Regression) is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. This set of parameters can be used for predicting the data needed in a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of Logistic Regression\n",
    "\n",
    "Types of Logistic Regression:\n",
    "\n",
    "**Binary Logistic Regression:** The target variable has only two possible outcomes such as Spam or Not Spam, Cancer or No Cancer.\n",
    "\n",
    "**Multinomial Logistic Regression:** The target variable has three or more nominal categories such as predicting the type of Wine.\n",
    "\n",
    "**Ordinal Logistic Regression:** the target variable has three or more ordinal categories such as product rating from 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression on Breast Cancer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43,  1],\n",
       "       [ 5, 94]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.958041958041958\n",
      "Precision: 0.9894736842105263\n",
      "Recall: 0.9494949494949495\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. \n",
    "\n",
    "* Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. \n",
    "\n",
    "* Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
