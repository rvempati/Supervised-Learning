{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is Boosting?\n",
    "\n",
    "Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree.\n",
    "When an input is misclassified by a hypothesis, its weight is increased so that next hypothesis is more likely to classify it correctly. By combining the whole set at the end converts weak learners into better performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is an extension over boosting method.\n",
    "\n",
    "Gradient Boosting = Gradient Descent + Boosting.\n",
    "\n",
    "It uses gradient descent algorithm which can optimize any differentiable loss function. An ensemble of trees are built one by one and individual trees are summed sequentially. Next tree tries to recover the loss (difference between actual and predicted values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "It is a method which comprises a vector of weights (or coefficients) where we calculate their partial derivative with respective to zero. The motive behind calculating their partial derivative is to find the local minima of the loss function (RSS), which is convex in nature. In simple words, gradient descent tries to optimize the loss function by tuning different values of coefficients to minimize the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/GradientDescent.jpg' alt='Gradient' style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of using Gradient Boosting technique:\n",
    "\n",
    "1. Supports different loss function.\n",
    "2. Works well with interactions.\n",
    "\n",
    "#### Disadvantages of using Gradient Boosting technique:\n",
    "\n",
    "1. Prone to over-fitting.\n",
    "2. Requires careful tuning of different hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How XGBoost helps \n",
    "\n",
    "The problem with most tree packages is that they don't take regularization issues very seriously - they allow to grow many very similar trees that can be also sometimes quite bushy. \n",
    "\n",
    "GBT tries to approach this problem by adding some regularization parameters. We can:\n",
    "- control tree structure (maximum depth, minimum samples per leaf),\n",
    "- control learning rate (shrinkage),\n",
    "- reduce variance by introducing randomness (stochastic gradient boosting - using random subsamples of instances and features)\n",
    "\n",
    "But it could be improved even further. Enter XGBoost.\n",
    "\n",
    "> **XGBoost** (*extreme gradient boosting*) is a **more regularized** version of Gradient Boosted Trees.\n",
    "\n",
    "It was develop by Tianqi Chen in C++ but also enables interfaces for Python, R, Julia. Used for supervised learning problem gave win to many Kaggle competitions.\n",
    "\n",
    "The main advantages:\n",
    "- good bias-variance (simple-predictive) trade-off \"out of the box\",\n",
    "- great computation speed,\n",
    "- package is evolving (author is willing to accept many PR from community)\n",
    "\n",
    "XGBoost's objective function is a sum of a specific loss function evaluated over all predictions and a sum of regularization term for all predictors ($K$ trees). In the formula $f_k$ means a prediction coming from k-th tree.\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{k=1}^{K} \\Omega (f_k)\n",
    "$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "First part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the their's scores.\n",
    "\n",
    "To optimize the objective a gradient descent is used, this leads to a problem of finding an optimal structure of the successive tree. More mathematics about the algorithm is not included in the scope of this course, but pretty decent informations can be found on the package [docs page](http://xgboost.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale = 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you are trying to install xgboost package on Mac Book please try below command\n",
    "\n",
    "conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data<a name='data' />\n",
    "We are going to use bundled [Agaricus](https://archive.ics.uci.edu/ml/datasets/Mushroom) dataset which can be downloaded from UCI ML repository site.\n",
    "\n",
    "> This data set records biological attributes of different mushroom species, and the target is to predict whether it is poisonous\n",
    "\n",
    "> This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom;\n",
    "\n",
    "It consist of 8124 instances, characterized by 127 attributes (both numeric and categorical). The target class is either 0 or 1 which means binary classification problem.\n",
    "\n",
    "> **Important**: XGBoost handles only numeric variables.\n",
    "\n",
    "Luckily all the data have alreay been pre-process for us. Categorical variables have been encoded, and all instances divided into train and test datasets. You will know how to do this on your own in later lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:54:09] 6513x127 matrix with 143286 entries loaded from data/agaricus.txt.train\n",
      "[22:54:09] 1611x127 matrix with 35442 entries loaded from data/agaricus.txt.test\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix('data/agaricus.txt.train')\n",
    "dtest = xgb.DMatrix('data/agaricus.txt.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 6513 rows and 127 columns\n",
      "<built-in method format of str object at 0x10e2b8e70>\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset contains {0} rows and {1} columns\".format(dtrain.num_row(), dtrain.num_col()))\n",
    "print(\"Test dataset contains {0} rows and {1} columns\".format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train possible labels: \n",
      "[0. 1.]\n",
      "\n",
      "Test possible labels: \n",
      "[0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train possible labels: \")\n",
    "print(np.unique(dtrain.get_label()))\n",
    "\n",
    "print(\"\\nTest possible labels: \")\n",
    "print(np.unique(dtest.get_label()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training parameters<a name='params' />\n",
    "Let's make the following assuptions and adjust algorithm parameters to it:\n",
    "- we are dealing with binary classification problem (`'objective':'binary:logistic'`),\n",
    "- we want shallow single trees with no more than 2 levels (`'max_depth':2`),\n",
    "- we don't any oupout (`'silent':1`),\n",
    "- we want algorithm to learn fast and aggressively (`'eta':1`),\n",
    "- we want to iterate only 5 rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'binary:logistic',\n",
    "    'max_depth':2,\n",
    "    'silent':1,\n",
    "    'eta':1\n",
    "}\n",
    "\n",
    "num_rounds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier<a name='train' />\n",
    "To train the classifier we simply pass to it a training dataset, parameters list and information about number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(params, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also observe performance on test dataset using `watchlist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-error:0.042831\ttrain-error:0.046522\n",
      "[1]\ttest-error:0.021726\ttrain-error:0.022263\n",
      "[2]\ttest-error:0.006207\ttrain-error:0.007063\n",
      "[3]\ttest-error:0.018001\ttrain-error:0.0152\n",
      "[4]\ttest-error:0.006207\ttrain-error:0.007063\n"
     ]
    }
   ],
   "source": [
    "watchlist  = [(dtest,'test'), (dtrain,'train')] # native interface only\n",
    "bst = xgb.train(params, dtrain, num_rounds, watchlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08073306, 0.92217326, 0.08073306, ..., 0.98059034, 0.01182149,\n",
       "       0.98059034], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_prob = bst.predict(dtest)\n",
    "preds_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate simple accuracy metric to verify the results. Of course validation should be performed accordingly to the dataset, but in this case accuracy is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted correctly: 1601/1611\n",
      "Error: 0.0062\n"
     ]
    }
   ],
   "source": [
    "labels = dtest.get_label()\n",
    "preds = preds_prob > 0.5 # threshold\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if (labels[i] == preds[i]):\n",
    "        correct += 1\n",
    "\n",
    "print('Predicted correctly: {0}/{1}'.format(correct, len(preds)))\n",
    "print('Error: {0:.4f}'.format(1-correct/len(preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
